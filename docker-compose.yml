services:
  knowledge-retrieval:
    build: .
    ports:
      - "8000:8000"
    volumes:
      # Data volumes
      - ./data:/app/data
      - ./vector_db:/app/vector_db
      - ./uploads:/app/uploads
      - ./logs:/app/logs
      # Model volumes for external model storage
      - ./models:/app/models
      - models_cache:/app/models/transformers_cache
      - huggingface_cache:/app/models/huggingface
      - torch_cache:/app/models/torch
    environment:
      - DATABASE_URL=sqlite:////app/data/knowledge_retrieval.db
      - VECTOR_DB_PATH=/app/vector_db
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - REDIS_URL=redis://redis:6379/0
      - MAX_FILE_SIZE_MB=1000
      - CSV_CHUNK_SIZE=10000
      - BATCH_SIZE=1000
      # Model cache paths
      - TRANSFORMERS_CACHE=/app/models/transformers_cache
      - HF_HOME=/app/models/huggingface
      - TORCH_HOME=/app/models/torch
      # GPU support
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # CUDA multiprocessing fix
      - CUDA_VISIBLE_DEVICES=0
      - OMP_NUM_THREADS=1
      # Model configuration
      - LLM_MODEL=microsoft/DialoGPT-medium
    depends_on:
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru

  celery-worker:
    build: .
    command: celery -A celery_app worker --loglevel=info --queues=file_processing,question_processing --pool=solo
    volumes:
      # Data volumes
      - ./data:/app/data
      - ./vector_db:/app/vector_db
      - ./uploads:/app/uploads
      - ./logs:/app/logs
      # Model volumes for external model storage
      - ./models:/app/models
      - models_cache:/app/models/transformers_cache
      - huggingface_cache:/app/models/huggingface
      - torch_cache:/app/models/torch
    environment:
      - DATABASE_URL=sqlite:////app/data/knowledge_retrieval.db
      - VECTOR_DB_PATH=/app/vector_db
      - REDIS_URL=redis://redis:6379/0
      - TRANSFORMERS_CACHE=/app/models/transformers_cache
      - HF_HOME=/app/models/huggingface
      - TORCH_HOME=/app/models/torch
      # GPU support
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # CUDA multiprocessing fix
      - CUDA_VISIBLE_DEVICES=0
      - OMP_NUM_THREADS=1
      # Model configuration
      - LLM_MODEL=microsoft/DialoGPT-medium
    depends_on:
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  redis_data:
  models_cache:
  huggingface_cache:
  torch_cache: